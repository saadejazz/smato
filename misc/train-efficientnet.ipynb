{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec0fe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saad\\miniconda3\\envs\\looking\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Saad\\miniconda3\\envs\\looking\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\Saad\\miniconda3\\envs\\looking\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f943a",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Data is loaded from images in the folder *\"dataset/\"*. There are two classes: *\"true\"* specifies images with the pedestrian using a smartphone, while *\"false\"* specifies images with the pedestrian not holding a smartphone. The images are loaded, shuffled and batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436e5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "IMAGE_DIR = \"dataset/\"\n",
    "MODEL_NAME = \"smato_efficientnet_v2m\"\n",
    "SAVE_DIR = f\"saved_models/{MODEL_NAME}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a828557b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13866 files belonging to 2 classes.\n",
      "Number of pedestrians with a smartphone:  3770\n",
      "Number of pedestrians without a smartphone:  10096\n",
      "Classes:  ['false', 'true']\n",
      "Total number of batches created:  217\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.image_dataset_from_directory(IMAGE_DIR,\n",
    "                                                      shuffle = False,\n",
    "                                                      seed = 142,\n",
    "                                                      batch_size = None,\n",
    "                                                      image_size = IMAGE_SIZE)\n",
    "CLASS_NAMES = dataset.class_names\n",
    "dataset_size = len(dataset)\n",
    "dataset = dataset.shuffle(buffer_size = dataset_size, seed = 142, reshuffle_each_iteration = False)\n",
    "# count images in each category\n",
    "counts = {\n",
    "    1: 0,\n",
    "    0: 0\n",
    "}\n",
    "for i in dataset:\n",
    "    _, cls = i\n",
    "    counts[int(cls)] += 1\n",
    "print(\"Number of pedestrians with a smartphone: \", counts[1])\n",
    "print(\"Number of pedestrians without a smartphone: \", counts[0])\n",
    "\n",
    "# batch the dataset\n",
    "dataset = dataset.batch(batch_size = BATCH_SIZE)\n",
    "print(\"Classes: \", CLASS_NAMES)\n",
    "print(\"Total number of batches created: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a7f9c",
   "metadata": {},
   "source": [
    "## Splitting Dataset\n",
    "The dataset is split into training, validation, and test set. 85% of the dataset is reserved for training, while the rest is divided into validation and test dataset in the ratio of 60:40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "040f3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation split\n",
    "num_batches = len(dataset)\n",
    "val_dataset = dataset.take(int(num_batches * 0.15))\n",
    "train_dataset = dataset.skip(int(num_batches * 0.15))\n",
    "\n",
    "# validation-test split\n",
    "num_val_batches = len(val_dataset)\n",
    "test_dataset = val_dataset.take(int(num_val_batches * 0.4))\n",
    "validation_dataset = val_dataset.skip(int(num_val_batches * 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e59cf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in Training Dataset:  185\n",
      "Number of batches in Validation Dataset:  20\n",
      "Number of batches in Test Dataset:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of batches in Training Dataset: \", len(train_dataset))\n",
    "print(\"Number of batches in Validation Dataset: \", len(validation_dataset))\n",
    "print(\"Number of batches in Test Dataset: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e7be352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffered prefetching\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481c702",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "A model is defined using MobileNet-V2 as the feature extractor, followed by three fully connected hidden layers. Batch Normalization is performed after each FC layer, and dropout is used to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b6b61e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transfer learning approach - EfficientNetV2\n",
    "\n",
    "# load model and remove the head - will act as feature extractor/image embedding\n",
    "transferred = keras.applications.EfficientNetV2M(include_top = False)\n",
    "\n",
    "# freeze all layers of the feature extractor\n",
    "transferred.trainable = False\n",
    "\n",
    "# form the keras mode\n",
    "inputs = keras.Input(IMAGE_SIZE + (3, ))\n",
    "\n",
    "x = tf.keras.applications.efficientnet_v2.preprocess_input(inputs)\n",
    "x = transferred(x, training = False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.6)(x)\n",
    "\n",
    "x = keras.layers.Dense(128)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "x = keras.layers.Dense(32)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "x = keras.layers.Dense(8)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Activation(\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = keras.layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "# build and show model\n",
    "model = keras.Model(inputs, x, name = MODEL_NAME)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdb2cb",
   "metadata": {},
   "source": [
    "## Metric for choosing best model: F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26f68eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall)/(precision + recall + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f553bb8",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "While training the model, only the best model is saved. This is decided by the metric accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd79223",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer = tf.keras.optimizers.Adam(),\n",
    "  loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "  metrics = ['acc', f1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3969937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.2371 - acc: 0.9032 - f1: 0.8099INFO:tensorflow:Assets written to: saved_models/smato_efficientnet_v2m\\assets\n",
      "185/185 [==============================] - 223s 1s/step - loss: 0.2371 - acc: 0.9032 - f1: 0.8099 - val_loss: 0.1803 - val_acc: 0.9422 - val_f1: 0.8891\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.2377 - acc: 0.9042 - f1: 0.8112INFO:tensorflow:Assets written to: saved_models/smato_efficientnet_v2m\\assets\n",
      "185/185 [==============================] - 222s 1s/step - loss: 0.2377 - acc: 0.9042 - f1: 0.8112 - val_loss: 0.1789 - val_acc: 0.9430 - val_f1: 0.8904\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.2323 - acc: 0.9071 - f1: 0.8163INFO:tensorflow:Assets written to: saved_models/smato_efficientnet_v2m\\assets\n",
      "185/185 [==============================] - 228s 1s/step - loss: 0.2323 - acc: 0.9071 - f1: 0.8163 - val_loss: 0.1778 - val_acc: 0.9438 - val_f1: 0.8928\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.2301 - acc: 0.9036 - f1: 0.8096INFO:tensorflow:Assets written to: saved_models/smato_efficientnet_v2m\\assets\n",
      "185/185 [==============================] - 227s 1s/step - loss: 0.2301 - acc: 0.9036 - f1: 0.8096 - val_loss: 0.1772 - val_acc: 0.9469 - val_f1: 0.8984\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 142s 552ms/step - loss: 0.2293 - acc: 0.9084 - f1: 0.8220 - val_loss: 0.1742 - val_acc: 0.9430 - val_f1: 0.8895\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 142s 554ms/step - loss: 0.2348 - acc: 0.9038 - f1: 0.8114 - val_loss: 0.1742 - val_acc: 0.9461 - val_f1: 0.8966\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 142s 554ms/step - loss: 0.2297 - acc: 0.9070 - f1: 0.8168 - val_loss: 0.1742 - val_acc: 0.9406 - val_f1: 0.8846\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 144s 556ms/step - loss: 0.2247 - acc: 0.9119 - f1: 0.8269 - val_loss: 0.1730 - val_acc: 0.9445 - val_f1: 0.8936\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 144s 560ms/step - loss: 0.2269 - acc: 0.9096 - f1: 0.8217 - val_loss: 0.1733 - val_acc: 0.9398 - val_f1: 0.8818\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 144s 560ms/step - loss: 0.2277 - acc: 0.9096 - f1: 0.8256 - val_loss: 0.1747 - val_acc: 0.9430 - val_f1: 0.8860\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 144s 559ms/step - loss: 0.2314 - acc: 0.9063 - f1: 0.8183 - val_loss: 0.1747 - val_acc: 0.9438 - val_f1: 0.8906\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 146s 568ms/step - loss: 0.2331 - acc: 0.9033 - f1: 0.8088 - val_loss: 0.1781 - val_acc: 0.9453 - val_f1: 0.8919\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 147s 568ms/step - loss: 0.2282 - acc: 0.9088 - f1: 0.8212 - val_loss: 0.1763 - val_acc: 0.9430 - val_f1: 0.8901\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 147s 568ms/step - loss: 0.2321 - acc: 0.9079 - f1: 0.8185 - val_loss: 0.1755 - val_acc: 0.9406 - val_f1: 0.8837\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 147s 571ms/step - loss: 0.2315 - acc: 0.9077 - f1: 0.8189 - val_loss: 0.1784 - val_acc: 0.9414 - val_f1: 0.8874\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 146s 567ms/step - loss: 0.2278 - acc: 0.9090 - f1: 0.8202 - val_loss: 0.1765 - val_acc: 0.9422 - val_f1: 0.8880\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 147s 571ms/step - loss: 0.2266 - acc: 0.9088 - f1: 0.8220 - val_loss: 0.1765 - val_acc: 0.9406 - val_f1: 0.8847\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 146s 568ms/step - loss: 0.2303 - acc: 0.9089 - f1: 0.8197 - val_loss: 0.1763 - val_acc: 0.9445 - val_f1: 0.8947\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 147s 572ms/step - loss: 0.2279 - acc: 0.9090 - f1: 0.8225 - val_loss: 0.1765 - val_acc: 0.9375 - val_f1: 0.8791\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 147s 572ms/step - loss: 0.2317 - acc: 0.9056 - f1: 0.8136 - val_loss: 0.1778 - val_acc: 0.9391 - val_f1: 0.8813\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 148s 573ms/step - loss: 0.2409 - acc: 0.9040 - f1: 0.8110 - val_loss: 0.1803 - val_acc: 0.9367 - val_f1: 0.8765\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 148s 575ms/step - loss: 0.2276 - acc: 0.9087 - f1: 0.8189 - val_loss: 0.1785 - val_acc: 0.9422 - val_f1: 0.8899\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 149s 577ms/step - loss: 0.2241 - acc: 0.9095 - f1: 0.8232 - val_loss: 0.1765 - val_acc: 0.9422 - val_f1: 0.8916\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 149s 576ms/step - loss: 0.2300 - acc: 0.9057 - f1: 0.8145 - val_loss: 0.1798 - val_acc: 0.9375 - val_f1: 0.8794\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 149s 578ms/step - loss: 0.2285 - acc: 0.9117 - f1: 0.8293 - val_loss: 0.1763 - val_acc: 0.9375 - val_f1: 0.8806\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 150s 583ms/step - loss: 0.2291 - acc: 0.9083 - f1: 0.8195 - val_loss: 0.1752 - val_acc: 0.9430 - val_f1: 0.8918\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 151s 581ms/step - loss: 0.2234 - acc: 0.9075 - f1: 0.8198 - val_loss: 0.1705 - val_acc: 0.9461 - val_f1: 0.8977\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 150s 584ms/step - loss: 0.2319 - acc: 0.9087 - f1: 0.8206 - val_loss: 0.1721 - val_acc: 0.9445 - val_f1: 0.8923\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - ETA: 0s - loss: 0.2242 - acc: 0.9087 - f1: 0.8193INFO:tensorflow:Assets written to: saved_models/smato_efficientnet_v2m\\assets\n",
      "185/185 [==============================] - 243s 1s/step - loss: 0.2242 - acc: 0.9087 - f1: 0.8193 - val_loss: 0.1685 - val_acc: 0.9523 - val_f1: 0.9102\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 151s 584ms/step - loss: 0.2301 - acc: 0.9100 - f1: 0.8232 - val_loss: 0.1730 - val_acc: 0.9469 - val_f1: 0.8987\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 152s 587ms/step - loss: 0.2224 - acc: 0.9115 - f1: 0.8268 - val_loss: 0.1673 - val_acc: 0.9477 - val_f1: 0.8984\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 153s 590ms/step - loss: 0.2286 - acc: 0.9086 - f1: 0.8197 - val_loss: 0.1699 - val_acc: 0.9469 - val_f1: 0.8981\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 152s 584ms/step - loss: 0.2261 - acc: 0.9092 - f1: 0.8221 - val_loss: 0.1691 - val_acc: 0.9445 - val_f1: 0.8944\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 151s 587ms/step - loss: 0.2286 - acc: 0.9095 - f1: 0.8222 - val_loss: 0.1726 - val_acc: 0.9492 - val_f1: 0.9014\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 151s 585ms/step - loss: 0.2203 - acc: 0.9130 - f1: 0.8290 - val_loss: 0.1735 - val_acc: 0.9484 - val_f1: 0.8993\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 151s 584ms/step - loss: 0.2264 - acc: 0.9075 - f1: 0.8177 - val_loss: 0.1749 - val_acc: 0.9438 - val_f1: 0.8904\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 151s 585ms/step - loss: 0.2204 - acc: 0.9112 - f1: 0.8258 - val_loss: 0.1700 - val_acc: 0.9477 - val_f1: 0.9014\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 151s 585ms/step - loss: 0.2249 - acc: 0.9090 - f1: 0.8224 - val_loss: 0.1714 - val_acc: 0.9484 - val_f1: 0.9021\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 154s 593ms/step - loss: 0.2237 - acc: 0.9107 - f1: 0.8246 - val_loss: 0.1693 - val_acc: 0.9469 - val_f1: 0.8982\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 152s 589ms/step - loss: 0.2189 - acc: 0.9112 - f1: 0.8272 - val_loss: 0.1675 - val_acc: 0.9461 - val_f1: 0.8971\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 152s 588ms/step - loss: 0.2277 - acc: 0.9098 - f1: 0.8250 - val_loss: 0.1711 - val_acc: 0.9492 - val_f1: 0.9035\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 152s 590ms/step - loss: 0.2198 - acc: 0.9095 - f1: 0.8226 - val_loss: 0.1696 - val_acc: 0.9484 - val_f1: 0.9029\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 152s 587ms/step - loss: 0.2150 - acc: 0.9144 - f1: 0.8332 - val_loss: 0.1668 - val_acc: 0.9438 - val_f1: 0.8942\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 152s 589ms/step - loss: 0.2220 - acc: 0.9117 - f1: 0.8266 - val_loss: 0.1660 - val_acc: 0.9477 - val_f1: 0.9006\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 152s 589ms/step - loss: 0.2233 - acc: 0.9116 - f1: 0.8255 - val_loss: 0.1675 - val_acc: 0.9469 - val_f1: 0.8990\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 152s 587ms/step - loss: 0.2211 - acc: 0.9142 - f1: 0.8348 - val_loss: 0.1700 - val_acc: 0.9453 - val_f1: 0.8957\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 153s 593ms/step - loss: 0.2147 - acc: 0.9161 - f1: 0.8357 - val_loss: 0.1705 - val_acc: 0.9422 - val_f1: 0.8883\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 154s 593ms/step - loss: 0.2291 - acc: 0.9090 - f1: 0.8233 - val_loss: 0.1720 - val_acc: 0.9445 - val_f1: 0.8926\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 154s 594ms/step - loss: 0.2216 - acc: 0.9141 - f1: 0.8323 - val_loss: 0.1733 - val_acc: 0.9430 - val_f1: 0.8892\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 154s 595ms/step - loss: 0.2174 - acc: 0.9149 - f1: 0.8330 - val_loss: 0.1697 - val_acc: 0.9469 - val_f1: 0.8982\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = SAVE_DIR,\n",
    "    save_weights_only = False,\n",
    "    monitor = 'val_f1',\n",
    "    mode = 'max',\n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data = validation_dataset,\n",
    "                    epochs = NUM_EPOCHS,\n",
    "                    callbacks = [model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07998eeb",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "The best trained model is loaded and the test dataset is used for evaluating performance on data never been used in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f3ddb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 54s 643ms/step - loss: 0.2020 - acc: 0.9297 - f1: 0.8772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20198650658130646, 0.9296875, 0.8771961331367493]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(SAVE_DIR, custom_objects = {\"f1\": f1})\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b2f07",
   "metadata": {},
   "source": [
    "## Example Images\n",
    "A bunch of images from the test dataset are chosen to show the performance of the model and to understand the reasons for misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "967adef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Retrieve a batch of images from the test set\n",
    "# image_batch, label_batch = test_dataset.shuffle(10, seed = None).take(1).as_numpy_iterator().next()\n",
    "# predictions = model.predict_on_batch(image_batch[: 32]).flatten()\n",
    "\n",
    "# predictions = np.where(predictions < 0.5, 0, 1)\n",
    "# label_batch = label_batch[: 32]\n",
    "\n",
    "# print('Predictions:\\n', predictions)\n",
    "# print('Labels:\\n', label_batch)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(32):\n",
    "    ax = plt.subplot(4, 8, i + 1)\n",
    "    plt.imshow(image_batch[i] * 255.0)\n",
    "    plt.title(str(CLASS_NAMES[predictions[i]]) + \", \" + str(label_batch[i]))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c7402a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
